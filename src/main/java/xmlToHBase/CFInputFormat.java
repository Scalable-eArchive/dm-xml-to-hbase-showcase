package xmlToHBase;import java.io.IOException;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.JobContext;import org.apache.hadoop.mapreduce.RecordReader;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat;import org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReader;import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;/** * Breaks the input file into specified chunks. The input file is a combined file. */public class CFInputFormat extends CombineFileInputFormat<FileLineWritable, Text> {    private static final int HDFS_DEFAULT_BLOCK_SIZE = 1024 * 1024 * 64;    public static final String START_TAG_KEY = "xmlinput.start";    public static final String END_TAG_KEY = "xmlinput.end";    public CFInputFormat() {        setMaxSplitSize(HDFS_DEFAULT_BLOCK_SIZE);    }    @Override    public RecordReader<FileLineWritable, Text> createRecordReader(InputSplit split, TaskAttemptContext context)            throws IOException {        return new CombineFileRecordReader<FileLineWritable, Text>((CombineFileSplit) split, context,                CFRecordReader.class);    }    @Override    protected boolean isSplitable(JobContext context, Path file) {        // do not split below our desired 64MB        return false;    }}